# 딥 러닝을 이용한 자연어 처리 입문

아래 링크의 E-book을 보고 실습한 내용입니다.

WikiDocs 주소: https://wikidocs.net/31766

# 2장 텍스트 전처리

## 7절 패딩 실습

## 패딩

문장 또는 문서의 길이가 다를 때 길이를 맞춰주어 한 꺼번에 묶어서 처리가 가능하게 하는 작업이다.

```python
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
```

```python
text = "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain."

# 문장 토큰화
text = sent_tokenize(text)

# 불용어를 제거하고 vocab에 단어 빈도수를 저장한다.
vocab = {}
sentences = []
stop_words = set(stopwords.words("english"))

for i in text:
    sentence = word_tokenize(i)
    result = []

    for word in sentence:
        word = word.lower()
        if word not in stop_words:
            if len(word) > 2:
                result.append(word)
                if word not in vocab:
                    vocab[word] = 0
                vocab[word] += 1
    sentences.append(result)
print(sentences)
```

    [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]

## Numpy로 패딩하기

```python
# 빈도수를 기준으로 단어 그룹화
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)

# 정수 인코딩
encoded = tokenizer.texts_to_sequences(sentences)
print(encoded)
```

    [[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]

```python
# 가장 길이가 긴 문장의 길이
max_len = max(len(item) for item in encoded)
print(max_len)
```

    7

```python
# 가장 긴 단어 7로 패딩하기
for item in encoded:
    while len(item) < max_len:
        item.append(0)

padded_np = np.array(encoded)
padded_np
```

    array([[ 1,  5,  0,  0,  0,  0,  0],
           [ 1,  8,  5,  0,  0,  0,  0],
           [ 1,  3,  5,  0,  0,  0,  0],
           [ 9,  2,  0,  0,  0,  0,  0],
           [ 2,  4,  3,  2,  0,  0,  0],
           [ 3,  2,  0,  0,  0,  0,  0],
           [ 1,  4,  6,  0,  0,  0,  0],
           [ 1,  4,  6,  0,  0,  0,  0],
           [ 1,  4,  2,  0,  0,  0,  0],
           [ 7,  7,  3,  2, 10,  1, 11],
           [ 1, 12,  3, 13,  0,  0,  0]])

## Keras 전처리 도구로 패딩하기

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

encoded = tokenizer.texts_to_sequences(sentences)
print(encoded)
```

    [[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]

```python
# 앞에 0을 채움
# padded = pad_sequences(encoded, padding="pre")
# padded
# 이전 예저랑 같게 하려면 post 모드로 뒤에 0을 채우면 됨
padded = pad_sequences(encoded, padding="post")
padded
```

    array([[ 1,  5,  0,  0,  0,  0,  0],
           [ 1,  8,  5,  0,  0,  0,  0],
           [ 1,  3,  5,  0,  0,  0,  0],
           [ 9,  2,  0,  0,  0,  0,  0],
           [ 2,  4,  3,  2,  0,  0,  0],
           [ 3,  2,  0,  0,  0,  0,  0],
           [ 1,  4,  6,  0,  0,  0,  0],
           [ 1,  4,  6,  0,  0,  0,  0],
           [ 1,  4,  2,  0,  0,  0,  0],
           [ 7,  7,  3,  2, 10,  1, 11],
           [ 1, 12,  3, 13,  0,  0,  0]])

```python
# 최대 길이가 7이지만 5로 맞출 수도 있음
pad_sequences(encoded, padding="post", maxlen=5)
```

    array([[ 1,  5,  0,  0,  0],
           [ 1,  8,  5,  0,  0],
           [ 1,  3,  5,  0,  0],
           [ 9,  2,  0,  0,  0],
           [ 2,  4,  3,  2,  0],
           [ 3,  2,  0,  0,  0],
           [ 1,  4,  6,  0,  0],
           [ 1,  4,  6,  0,  0],
           [ 1,  4,  2,  0,  0],
           [ 3,  2, 10,  1, 11],
           [ 1, 12,  3, 13,  0]])

```python
# 숫자 0이 아닌 다른 값으로 padding 하기
last_index = len(tokenizer.word_index) + 1
print(last_index)

padded = pad_sequences(encoded, padding="post", value=last_index)
padded
```

    14





    array([[ 1,  5, 14, 14, 14, 14, 14],
           [ 1,  8,  5, 14, 14, 14, 14],
           [ 1,  3,  5, 14, 14, 14, 14],
           [ 9,  2, 14, 14, 14, 14, 14],
           [ 2,  4,  3,  2, 14, 14, 14],
           [ 3,  2, 14, 14, 14, 14, 14],
           [ 1,  4,  6, 14, 14, 14, 14],
           [ 1,  4,  6, 14, 14, 14, 14],
           [ 1,  4,  2, 14, 14, 14, 14],
           [ 7,  7,  3,  2, 10,  1, 11],
           [ 1, 12,  3, 13, 14, 14, 14]])
