{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1925e5f",
   "metadata": {},
   "source": [
    "# 딥 러닝을 이용한 자연어 처리 입문\n",
    "\n",
    "아래 링크의 E-book을 보고 실습한 내용입니다.\n",
    "\n",
    "WikiDocs 주소: https://wikidocs.net/31766\n",
    "\n",
    "\n",
    "# 2장 텍스트 전처리\n",
    "\n",
    "## 8절 One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad934226",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "문자를 컴퓨터가 잘 처리할 수 있는 숫자 형태로 바꾸는 기법 중 하나이다. \n",
    "\n",
    "One-hot encoding에서는 단어 집합의 단어들을 벡터로 바꾸는 기법으로 여기서 단어 집합이란 서로 다른 단어들의 집합을 말한다. 단어 집합에서는 표기법이 다른 경우(ex. 'book', 'books')에는 서로 다른 단어로 간주한다. \n",
    "\n",
    "벡터로 만드는 과정은 간단하게 표현하고 싶은 단어의 인덱스에만 1을 부여하고 나머지에는 0을 부여하면 된다. 단어의 인덱스가 필요하기 때문에 텍스트의 모든 단어들을 중복을 허용하지 않고 모아 단어 집합을 만들고 정수 인코딩을 하는 과정이 필요하다.\n",
    "\n",
    "### One-hot encoding의 한계\n",
    "\n",
    "- 단어 집합의 크기가 곧 벡터의 차원이 되기 때문에 저장 공간 측면에서 매우 비효율적인 방법이다. \n",
    "- 단어의 유사도를 표현하지 못한다. (ex. 강아지와 늑대, 고양이와 호랑이가 유사함을 표현할 수 없다.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a055a14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
    "text = sent_tokenize(text)\n",
    "\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for i in text:\n",
    "    # 문장 토큰화\n",
    "    sentence = word_tokenize(i)\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        # 소문자화\n",
    "        word = word.lower()\n",
    "        # 불용어 제거\n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2:\n",
    "                result.append(word)\n",
    "    sentences.append(result)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "print(tokenizer.word_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110304d",
   "metadata": {},
   "source": [
    "### One-hot encoding 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b85fe048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 2, 'person': 1, 'good': 2, 'huge': 3, 'knew': 4, 'secret': 5, 'kept': 6, 'word': 7, 'keeping': 8, 'driving': 9, 'crazy': 10, 'went': 11, 'mountain': 12}\n",
      "\n",
      "One-hot encoding 결과:  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding\n",
    "def one_hot_encoding(word, word2Index):\n",
    "    \"\"\"\n",
    "    @inputs\n",
    "        word: array of string\n",
    "        word2Index: dictionary of word, frequency pairs\n",
    "    \"\"\"\n",
    "    # 0은 예약된 index\n",
    "    one_hot_vector = [0] * len(word2Index)\n",
    "    one_hot_vector[word2Index[word.lower()]] = 1\n",
    "    return one_hot_vector\n",
    "                            \n",
    "word2index={}\n",
    "for voca in sum(sentences, []):\n",
    "    voca = voca.lower()\n",
    "    if not word2index.get(voca):\n",
    "        word2index[voca]=len(word2index)\n",
    "print(word2index)\n",
    "\n",
    "one_hot = one_hot_encoding(\"huge\", word2index)\n",
    "print(\"\\nOne-hot encoding 결과: \", one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45968c97",
   "metadata": {},
   "source": [
    "\n",
    "### Keras로 One-hot encoding 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a4728e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
      "\n",
      "인코딩 된 문장:  [[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
      "\n",
      "One-hot encoding 결과:\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 정수 인코딩\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "# 문장을 인코딩\n",
    "encoded = tokenizer.texts_to_sequences(text)\n",
    "print(\"\\n인코딩 된 문장: \", encoded)\n",
    "\n",
    "# one hot encoding\n",
    "one_hot = to_categorical(sum(encoded, []))\n",
    "print(\"\\nOne-hot encoding 결과:\\n\", one_hot)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "131a2fe7960016a5e2629e09f3ca62b0c2ece83e5ed32d3907f8ac9bf090694a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
