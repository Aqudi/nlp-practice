{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1925e5f",
   "metadata": {},
   "source": [
    "# 딥 러닝을 이용한 자연어 처리 입문\n",
    "\n",
    "아래 링크의 E-book을 보고 실습한 내용입니다.\n",
    "\n",
    "WikiDocs 주소: https://wikidocs.net/31766\n",
    "\n",
    "\n",
    "# 2장 텍스트 전처리\n",
    "\n",
    "## 8절 One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad934226",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "문자를 컴퓨터가 잘 처리할 수 있는 숫자 형태로 바꾸는 기법 중 하나이다. \n",
    "\n",
    "One-hot encoding에서는 단어 집합의 단어들을 벡터로 바꾸는 기법으로 여기서 단어 집합이란 서로 다른 단어들의 집합을 말한다. 단어 집합에서는 표기법이 다른 경우(ex. 'book', 'books')에는 서로 다른 단어로 간주한다. \n",
    "\n",
    "벡터로 만드는 과정은 간단하게 표현하고 싶은 단어의 인덱스에만 1을 부여하고 나머지에는 0을 부여하면 된다. 단어의 인덱스가 필요하기 때문에 텍스트의 모든 단어들을 중복을 허용하지 않고 모아 단어 집합을 만들고 정수 인코딩을 하는 과정이 필요하다.\n",
    "\n",
    "### One-hot encoding의 한계\n",
    "\n",
    "- 단어 집합의 크기가 곧 벡터의 차원이 되기 때문에 저장 공간 측면에서 매우 비효율적인 방법이다. \n",
    "- 단어의 유사도를 표현하지 못한다. (ex. 강아지와 늑대, 고양이와 호랑이가 유사함을 표현할 수 없다.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5224202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a055a14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', '.'], ['Phasellus', 'porta', ',', 'dolor', 'at', 'imperdiet', 'pretium', ',', 'ante', 'justo', 'pulvinar', 'tortor', ',', 'at', 'faucibus', 'purus', 'sem', 'rutrum', 'urna', '.'], ['Vestibulum', 'et', 'ornare', 'enim', '.'], ['Quisque', 'sit', 'amet', 'ipsum', 'eu', 'diam', 'vestibulum', 'feugiat', 'sit', 'amet', 'et', 'ipsum', '.'], ['In', 'augue', 'eros', ',', 'consectetur', 'vel', 'fringilla', 'aliquam', ',', 'ullamcorper', 'vitae', 'mauris', '.'], ['Nulla', 'vitae', 'ante', 'quis', 'risus', 'tincidunt', 'tempor', '.'], ['Nunc', 'facilisis', 'fringilla', 'purus', 'eu', 'sodales', '.'], ['Suspendisse', 'lacus', 'erat', ',', 'sodales', 'feugiat', 'orci', 'ac', ',', 'vulputate', 'rutrum', 'mi', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 대상 텍스트\n",
    "text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus porta, dolor at imperdiet pretium, ante justo pulvinar tortor, at faucibus purus sem rutrum urna. Vestibulum et ornare enim. Quisque sit amet ipsum eu diam vestibulum feugiat sit amet et ipsum. In augue eros, consectetur vel fringilla aliquam, ullamcorper vitae mauris. Nulla vitae ante quis risus tincidunt tempor. Nunc facilisis fringilla purus eu sodales. Suspendisse lacus erat, sodales feugiat orci ac, vulputate rutrum mi.\"\n",
    "\n",
    "# 토큰화 과정\n",
    "sentences = sent_tokenize(text)\n",
    "sentences = [word_tokenize(sent) for sent in sentences]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110304d",
   "metadata": {},
   "source": [
    "### One-hot encoding 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b85fe048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lorem': 0, 'ipsum': 1, 'dolor': 2, 'sit': 3, 'amet': 4, ',': 5, 'consectetur': 6, 'adipiscing': 7, 'elit': 8, '.': 9, 'phasellus': 10, 'porta': 11, 'at': 12, 'imperdiet': 13, 'pretium': 14, 'ante': 15, 'justo': 16, 'pulvinar': 17, 'tortor': 18, 'faucibus': 19, 'purus': 20, 'sem': 21, 'rutrum': 22, 'urna': 23, 'vestibulum': 24, 'et': 25, 'ornare': 26, 'enim': 27, 'quisque': 28, 'eu': 29, 'diam': 30, 'feugiat': 31, 'in': 32, 'augue': 33, 'eros': 34, 'vel': 35, 'fringilla': 36, 'aliquam': 37, 'ullamcorper': 38, 'vitae': 39, 'mauris': 40, 'nulla': 41, 'quis': 42, 'risus': 43, 'tincidunt': 44, 'tempor': 45, 'nunc': 46, 'facilisis': 47, 'sodales': 48, 'suspendisse': 49, 'lacus': 50, 'erat': 51, 'orci': 52, 'ac': 53, 'vulputate': 54, 'mi': 55}\n",
      "\n",
      "One-hot encoding 결과:  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding\n",
    "def one_hot_encoding(word, word2Index):\n",
    "    # 0은 예약된 index\n",
    "    one_hot_vector = [0] * len(word2Index)\n",
    "    one_hot_vector[word2Index[word.lower()]] = 1\n",
    "    return one_hot_vector\n",
    "                            \n",
    "word2index={}\n",
    "for voca in sum(sentences, []):\n",
    "    voca = voca.lower()\n",
    "    if not word2index.get(voca):\n",
    "        word2index[voca]=len(word2index)\n",
    "print(word2index)\n",
    "\n",
    "one_hot = one_hot_encoding(\"dolor\", word2index)\n",
    "print(\"\\nOne-hot encoding 결과: \", one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45968c97",
   "metadata": {},
   "source": [
    "\n",
    "### Keras로 One-hot encoding 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0a4728e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': 1, '.': 2, 'ipsum': 3, 'sit': 4, 'amet': 5, 'dolor': 6, 'consectetur': 7, 'at': 8, 'ante': 9, 'purus': 10, 'rutrum': 11, 'vestibulum': 12, 'et': 13, 'eu': 14, 'feugiat': 15, 'fringilla': 16, 'vitae': 17, 'sodales': 18, 'lorem': 19, 'adipiscing': 20, 'elit': 21, 'phasellus': 22, 'porta': 23, 'imperdiet': 24, 'pretium': 25, 'justo': 26, 'pulvinar': 27, 'tortor': 28, 'faucibus': 29, 'sem': 30, 'urna': 31, 'ornare': 32, 'enim': 33, 'quisque': 34, 'diam': 35, 'in': 36, 'augue': 37, 'eros': 38, 'vel': 39, 'aliquam': 40, 'ullamcorper': 41, 'mauris': 42, 'nulla': 43, 'quis': 44, 'risus': 45, 'tincidunt': 46, 'tempor': 47, 'nunc': 48, 'facilisis': 49, 'suspendisse': 50, 'lacus': 51, 'erat': 52, 'orci': 53, 'ac': 54, 'vulputate': 55, 'mi': 56}\n",
      "\n",
      "인코딩 된 문장:  [[19, 3, 6, 4, 5, 7, 20, 21, 22, 23, 6, 8, 24, 25, 9, 26, 27, 28, 8, 29, 10, 30, 11, 31, 12, 13, 32, 33, 34, 4, 5, 3, 14, 35, 12, 15, 4, 5, 13, 3, 36, 37, 38, 7, 39, 16, 40, 41, 17, 42, 43, 17, 9, 44, 45, 46, 47, 48, 49, 16, 10, 14, 18, 50, 51, 52, 18, 15, 53, 54, 55, 11, 56]]\n",
      "\n",
      "One-hot encoding 결과:  [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 1. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 정수 인코딩\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "# 문장을 인코딩\n",
    "encoded = tokenizer.texts_to_sequences([text])\n",
    "print(\"\\n인코딩 된 문장: \", encoded)\n",
    "\n",
    "# one hot encoding\n",
    "one_hot = to_categorical(encoded)\n",
    "print(\"\\nOne-hot encoding 결과: \", one_hot)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb9f406c0f70fca9801e60f2cbb7cd1ccff2ae2f74c58f513340bcf6cae5ecd0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
