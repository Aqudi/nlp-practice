{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥 러닝을 이용한 자연어 처리 입문\n",
    "\n",
    "아래 링크의 E-book을 보고 실습한 내용입니다.\n",
    "\n",
    "WikiDocs 주소: https://wikidocs.net/31766\n",
    "\n",
    "# 6장 토픽 모델링\n",
    "\n",
    "## 2절 잠재 디리클레 할당 (Latent Dirichlet Allocation, LDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴스 그룹 데이터를 활용한 LDA 실습 - scikit learn\n",
    "\n",
    "### 뉴스 제목 데이터 다운로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터의 개수: 1082168\n",
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n"
     ]
    }
   ],
   "source": [
    "print(\"데이터의 개수:\", len(data))\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 전처리\n",
    "- 불용어 제거\n",
    "- 표제어 추출\n",
    "- 길이가 짧은 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decides, against, community, broadcastin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witnesses, must, be, aware, of, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[a, g, calls, for, infrastructure, protection,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, nz, staff, in, aust, strike, for, pay, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, nz, strike, to, affect, australian, trav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  [aba, decides, against, community, broadcastin...\n",
       "1  [act, fire, witnesses, must, be, aware, of, de...\n",
       "2  [a, g, calls, for, infrastructure, protection,...\n",
       "3  [air, nz, staff, in, aust, strike, for, pay, r...\n",
       "4  [air, nz, strike, to, affect, australian, trav..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decide, against, community, broadcast, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witness, must, be, aware, of, defa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[a, g, call, for, infrastructure, protection, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, nz, staff, in, aust, strike, for, pay, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, nz, strike, to, affect, australian, trav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  [aba, decide, against, community, broadcast, l...\n",
       "1  [act, fire, witness, must, be, aware, of, defa...\n",
       "2  [a, g, call, for, infrastructure, protection, ...\n",
       "3  [air, nz, staff, in, aust, strike, for, pay, r...\n",
       "4  [air, nz, strike, to, affect, australian, trav..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "길이가 짧은 단어 제거\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [decide, against, community, broadcast, licence]\n",
       "1            [fire, witness, must, aware, defamation]\n",
       "2          [call, infrastructure, protection, summit]\n",
       "3                         [staff, aust, strike, rise]\n",
       "4            [strike, affect, australian, travellers]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from IPython.display import display\n",
    "\n",
    "# 헤드라인만 따로 저장\n",
    "text = data[['headline_text']].copy()\n",
    "\n",
    "# 토큰화\n",
    "text.loc[:, 'headline_text'] = text.apply(\n",
    "    lambda row: nltk.word_tokenize(row['headline_text']), axis=1)\n",
    "print(\"토큰화\")\n",
    "display(text.head(5))\n",
    "\n",
    "# 표제어 추출\n",
    "text.loc[:, 'headline_text'] = text.loc[:, 'headline_text'].apply(\n",
    "    lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "print(\"표제어 추출\")\n",
    "display(text.head(5))\n",
    "\n",
    "# 길이가 짧은 단어 제거\n",
    "tokenized_doc = text.loc[:, 'headline_text'].apply(\n",
    "    lambda x: [word for word in x if len(word) > 3])\n",
    "print(\"길이가 짧은 단어 제거\")\n",
    "display(tokenized_doc[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 행렬 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    decide against community broadcast licence\n",
       "1            fire witness must aware defamation\n",
       "2         call infrastructure protection summit\n",
       "3                        staff aust strike rise\n",
       "4           strike affect australian travellers\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' ' .join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "text['headline_text'] = detokenized_doc\n",
    "text['headline_text'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082168, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(text['headline_text'])\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=10, learning_method='online', random_state=2, max_iter=1)\n",
    "lda_top = lda_model.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components shape: (10, 1000)\n",
      "토픽 내의 단어들의 분포\n",
      " [[1.00000871e-01 1.00003339e-01 1.00014905e-01 ... 1.00004036e-01\n",
      "  1.00002263e-01 1.00003288e-01]\n",
      " [1.00002227e-01 1.00002129e-01 1.00005836e-01 ... 1.00008125e-01\n",
      "  1.00003007e-01 1.00003514e-01]\n",
      " [3.51600415e+02 1.13513398e+03 1.00010754e-01 ... 1.00004829e-01\n",
      "  1.00002995e-01 1.00003080e-01]\n",
      " ...\n",
      " [1.00001571e-01 1.00001298e-01 3.50170828e+03 ... 1.77619510e+03\n",
      "  1.50652739e+02 1.00004157e-01]\n",
      " [1.00000922e-01 1.00000856e-01 1.00003222e-01 ... 1.00004189e-01\n",
      "  1.00005413e-01 1.00002368e-01]\n",
      " [1.00001079e-01 1.00001123e-01 1.00003873e-01 ... 1.00003931e-01\n",
      "  1.00001535e-01 1.00003452e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Components shape:\", lda_model.components_.shape)\n",
    "print(\"토픽 내의 단어들의 분포\\n\", lda_model.components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('police', 12092.44), ('government', 8725.19), ('sydney', 8393.29), ('south', 6677.03), ('help', 5225.56)]\n",
      "Topic 2: [('trump', 11966.41), ('attack', 6959.64), ('change', 5874.27), ('year', 5586.42), ('china', 4533.32)]\n",
      "Topic 3: [('australian', 11088.95), ('charge', 8428.8), ('queensland', 7720.12), ('world', 6707.7), ('murder', 6268.13)]\n",
      "Topic 4: [('melbourne', 7528.43), ('canberra', 6112.23), ('plan', 6033.16), ('live', 5488.62), ('brisbane', 4857.21)]\n",
      "Topic 5: [('2016', 5488.19), ('crash', 5281.14), ('state', 4923.41), ('people', 4121.07), ('national', 4038.68)]\n",
      "Topic 6: [('australia', 13691.08), ('coast', 5429.41), ('woman', 3909.11), ('leave', 3849.71), ('gold', 3793.71)]\n",
      "Topic 7: [('election', 7561.63), ('adelaide', 6758.36), ('death', 5935.06), ('home', 5674.38), ('make', 5658.99)]\n",
      "Topic 8: [('warn', 5115.01), ('tasmanian', 4859.02), ('jail', 4632.85), ('turnbull', 4269.85), ('women', 4232.53)]\n",
      "Topic 9: [('court', 7542.74), ('perth', 6456.53), ('house', 6113.49), ('open', 5663.0), ('school', 5465.06)]\n",
      "Topic 10: [('interview', 5924.98), ('kill', 5851.6), ('market', 5545.86), ('shoot', 4492.07), ('time', 3966.8)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (\n",
    "            idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "\n",
    "\n",
    "get_topics(lda_model.components_, terms)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "131a2fe7960016a5e2629e09f3ca62b0c2ece83e5ed32d3907f8ac9bf090694a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
