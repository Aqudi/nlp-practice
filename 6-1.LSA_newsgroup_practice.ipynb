{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 딥 러닝을 이용한 자연어 처리 입문\r\n",
    "\r\n",
    "아래 링크의 E-book을 보고 실습한 내용입니다.\r\n",
    "\r\n",
    "WikiDocs 주소: https://wikidocs.net/31766\r\n",
    "\r\n",
    "\r\n",
    "# 6장 토픽 모델링\r\n",
    "\r\n",
    "## 1절 잠재 의미 분석 (Latent Semantic Analysis, LSA)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 뉴스 그룹 데이터를 활용한 LSA 실습"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pandas as pd\r\n",
    "from sklearn.datasets import fetch_20newsgroups\r\n",
    "\r\n",
    "dataset = fetch_20newsgroups(\r\n",
    "    shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\r\n",
    "documents = dataset.data\r\n",
    "\r\n",
    "print(\"뉴스 그룹 데이터의 개수:\", len(documents))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "뉴스 그룹 데이터의 개수: 11314\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(\"뉴스 그룹 데이터 예시 1:\\n\", documents[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "뉴스 그룹 데이터 예시 1:\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yeah, do you expect people to read the FAQ, etc. and actually accept hard\n",
      "atheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\n",
      "of steam!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jim,\n",
      "\n",
      "Sorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\n",
      "denial about the faith you need to get by.  Oh well, just pretend that it will\n",
      "all end happily ever after anyway.  Maybe if you start a new newsgroup,\n",
      "alt.atheist.hard, you won't be bummin' so much?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \n",
      "--\n",
      "Bake Timmons, III\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "print(\"뉴스 그룹 데이터가 가지고 있는 카테고리:\\n\", dataset.target_names)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "뉴스 그룹 데이터가 가지고 있는 카테고리:\n",
      " ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 텍스트 전처리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "news_df = pd.DataFrame({\"document\": documents})\r\n",
    "\r\n",
    "# 특수 문자 제거 - 영문자가 아니면 모두 공백으로 치환한다.\r\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\r\n",
    "    r\"[^a-zA-Z]\", \" \", regex=True)\r\n",
    "\r\n",
    "# 길이가 짧은 단어 제거 - 길이가 3이하면 제거\r\n",
    "# 단어들 모두 소문자로 변환\r\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(\r\n",
    "    lambda x: ' '.join([w.lower() for w in x.split() if len(w) > 3]))\r\n",
    "\r\n",
    "# 데이터 정제 후 결과 확인\r\n",
    "news_df['clean_doc'][1]\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# 토큰화 수행 후 불용어 제거\r\n",
    "from nltk.corpus import stopwords\r\n",
    "\r\n",
    "stop_words = stopwords.words('english')\r\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\r\n",
    "tokenized_doc = tokenized_doc.apply(\r\n",
    "    lambda x: [item for item in x if item not in stop_words])\r\n",
    "\r\n",
    "# 불용어 제거 후 토큰화 되어 있는 결과 확인\r\n",
    "print(tokenized_doc[1])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['yeah', 'expect', 'people', 'read', 'actually', 'accept', 'hard', 'atheism', 'need', 'little', 'leap', 'faith', 'jimmy', 'logic', 'runs', 'steam', 'sorry', 'pity', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well', 'pretend', 'happily', 'ever', 'anyway', 'maybe', 'start', 'newsgroup', 'atheist', 'hard', 'bummin', 'much', 'forget', 'flintstone', 'chewables', 'bake', 'timmons']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# TF-IDF 행렬을 만들기 위해 역토큰화 진행\r\n",
    "detokenized_doc = []\r\n",
    "for i in range(len(news_df)):\r\n",
    "    t = ' '.join(tokenized_doc[i])\r\n",
    "    detokenized_doc.append(t)\r\n",
    "news_df['clean_doc'] = detokenized_doc\r\n",
    "\r\n",
    "# 역토큰화 결과 확인\r\n",
    "news_df['clean_doc'][1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# TF-IDF 행렬 만들기\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "\r\n",
    "vectorizer = TfidfVectorizer(\r\n",
    "    stop_words='english', \r\n",
    "    max_features=1000, # 상위 1000개 단어만 보존 \r\n",
    "    max_df = 0.5,\r\n",
    "    smooth_idf=True,\r\n",
    ")\r\n",
    "\r\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\r\n",
    "print(\"X의 크기:\", X.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X의 크기: (11314, 1000)\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 토픽 모델링\r\n",
    "\r\n",
    "- 토픽 모델링을 통해 해당 문서의 주제를 파악해본다.  \r\n",
    "- 본래 20개의 카테고리를 가졌기 때문에 20개의 토픽이 존재한다고 가정하고 진행한다.\r\n",
    "\r\n",
    "\r\n",
    "- scikit-learn의 TruncatedSVD를 사용하여 차원축소 및 토픽 모델링을 시도한다."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "from sklearn.decomposition import TruncatedSVD\r\n",
    "\r\n",
    "svd = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\r\n",
    "svd.fit(X)\r\n",
    "print(\"토픽의 수:\",len(svd.components_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "토픽의 수: 20\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "print(\"SVD의 V^T에 해당하는 svd.components_,\\nshape:\", np.shape(svd.components_), \"= (토픽의 수, 단어의 수)\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVD의 V^T에 해당하는 svd.components_,\n",
      "shape: (20, 1000) = (토픽의 수, 단어의 수)\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# 단어들의 집합, 1000개의 단어 (위에서 상위 1000개만 보존하도록 설정함)\r\n",
    "terms = vectorizer.get_feature_names()\r\n",
    "\r\n",
    "\r\n",
    "def get_topics(components, feature_names, n=5):\r\n",
    "    for idx, topic in enumerate(components):\r\n",
    "        features = [(feature_names[i], topic[i].round(5))\r\n",
    "                    for i in topic.argsort()[:-n - 1: -1]]\r\n",
    "        print(f\"Topic: {idx+1},\\t{features}\")\r\n",
    "\r\n",
    "\r\n",
    "get_topics(svd.components_, terms)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 1,\t[('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
      "Topic: 2,\t[('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n",
      "Topic: 3,\t[('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n",
      "Topic: 4,\t[('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n",
      "Topic: 5,\t[('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n",
      "Topic: 6,\t[('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n",
      "Topic: 7,\t[('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n",
      "Topic: 8,\t[('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n",
      "Topic: 9,\t[('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n",
      "Topic: 10,\t[('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n",
      "Topic: 11,\t[('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n",
      "Topic: 12,\t[('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n",
      "Topic: 13,\t[('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n",
      "Topic: 14,\t[('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n",
      "Topic: 15,\t[('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n",
      "Topic: 16,\t[('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n",
      "Topic: 17,\t[('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n",
      "Topic: 18,\t[('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n",
      "Topic: 19,\t[('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n",
      "Topic: 20,\t[('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "069592fba0e9980ecc5a793976b41e99d874b4a5f6b9a9400962961e715149e8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit (system)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}